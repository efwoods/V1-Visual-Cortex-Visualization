# config.yaml
latent_dim: 128
batch_size: 32
learning_rate: 1e-4
epochs: 100
save_every: 10
checkpoint_dir: checkpoints/

waveform_dict_path: data/collected_stimulus_waveforms.pkl
image_paths_dict_path: data/image_paths_dict.pkl

---

# models/waveform_encoder.py
import torch.nn as nn


class WaveformEncoder(nn.Module):
    def __init__(self, latent_dim=128):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Flatten(), nn.Linear(16 * 48, 256), nn.ReLU(), nn.Linear(256, latent_dim)
        )

    def forward(self, x):
        return self.encoder(x)

---

# models/waveform_decoder.py
import torch.nn as nn


class WaveformDecoder(nn.Module):
    def __init__(self, latent_dim=128, out_shape=(16, 48)):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(latent_dim, 512),
            nn.ReLU(),
            nn.Linear(512, out_shape[0] * out_shape[1]),
        )
        self.out_shape = out_shape

    def forward(self, z):
        x = self.fc(z)
        return x.view(-1, *self.out_shape)

---

# models/image_decoder.py
import torch.nn as nn


class ImageDecoder(nn.Module):
    def __init__(self, latent_dim=128, out_channels=3):
        super().__init__()
        self.fc = nn.Sequential(nn.Linear(latent_dim, 8 * 8 * 64), nn.ReLU())
        self.deconv = nn.Sequential(
            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 16x16
            nn.ReLU(),
            nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1),  # 32x32
            nn.ReLU(),
            nn.ConvTranspose2d(16, out_channels, 4, stride=2, padding=1),  # 64x64
            nn.Sigmoid(),
        )

    def forward(self, z):
        x = self.fc(z).view(-1, 64, 8, 8)
        return self.deconv(x)


---

# models/image_encoder.py
import torch.nn as nn
import torchvision.models as models


class ImageEncoder(nn.Module):
    def __init__(self, latent_dim=128):
        super().__init__()
        base = models.resnet18(weights="IMAGENET1K_V1")
        self.features = nn.Sequential(*list(base.children())[:-1])  # remove classifier
        self.fc = nn.Linear(base.fc.in_features, latent_dim)

    def forward(self, x):
        x = self.features(x).squeeze()
        return self.fc(x)


---

# utils.py
import torch.nn.functional as F


def latent_alignment_loss(z_image, z_waveform):
    return F.mse_loss(z_image, z_waveform)


---

# data/dataset.py
import os
from PIL import Image
import numpy as np
import torch
from torch.utils.data import Dataset


class ImageWaveformDataset(Dataset):
    def __init__(self, waveform_dict, image_paths_dict, transform=None):
        self.samples = []
        self.transform = transform

        for c in range(120):
            c_key = f"condition_{c:03d}"
            for e in range(16):
                e_key = f"electrode_{e:02d}"
                waveforms = waveform_dict[c][e]
                image_paths = image_paths_dict[c_key][e_key]
                for i in range(waveforms.shape[0]):
                    self.samples.append((waveforms[i], image_paths[i]))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        waveform, image_path = self.samples[idx]
        image = Image.open(image_path).convert("RGB")
        if self.transform:
            image = self.transform(image)
        waveform = torch.tensor(waveform, dtype=torch.float32)  # shape (16, 48)
        return image, waveform

---


---

## Data Flow Diagram

```
     (Train Phase)
   ┌──────────────┐
   │  Image Input │
   └──────┬───────┘
          ▼
  ┌─────────────────┐
  │  Image Encoder  │  (ResNet / ViT)
  └─────────────────┘
          ▼
    z_image_latent ───────┐
                          ▼
              ┌────────────────────┐
              │  Waveform Decoder  │ (MLP or 1D CNN)
              └────────────────────┘
                          ▼
                 Synthetic Waveform
                          ▼
              ┌────────────────────┐
              │  Waveform Encoder  │
              └────────────────────┘
                          ▼
    z_waveform_latent ◄────── latent alignment loss ──────► z_image_latent
                          ▼
              ┌──────────────────┐
              │  Image Decoder   │
              └──────────────────┘
                          ▼
               Reconstructed Image
```

```
Simulation Path (How to See)
Image ─▶ Image Encoder ─▶ z_image_latent ─▶ Waveform Decoder ─▶ Synthetic Waveform
```

```
Reconstruction Path (How to Visualize Sight, Imagination, and Dreams)
Waveform ─▶ Waveform Encoder ─▶ z_waveform_latent ─▶ Image Decoder ─▶ Reconstructed Image
```

## Model Project Architecture
```
project/
├── data/
│   ├── dataset.py             # Image + waveform loader
├── models/
│   ├── image_encoder.py       # CNN (ResNet) image → z
│   ├── waveform_decoder.py    # MLP z → waveform
│   ├── waveform_encoder.py    # MLP waveform → z
│   ├── image_decoder.py       # CNN decoder z → image
│   ├── __init__.py            # Shared architecture utils
├── train.py                   # Trains everything (2 phases)
├── eval.py                    # Runs SSIM, PSNR, MSE
├── config.yaml                # Configurable hyperparams
├── utils.py                   # Logger, metrics, visualizer
└── README.md                  # Usage + dependencies
```


The ImageWaveformDataset does not have images. They are file paths and numpy waveforms. 

There are approximately .5 million samples of numpy waveforms and image file paths. Each image is of size (320, 240). To load all the images will be memory inefficient. Please load the data in batches by loading the images and waveforms in batches using the file paths provided at each sample and use those to train the neural network. 

I need train.py. 